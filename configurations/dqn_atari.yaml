# env setting
env_id: ALE/Pong-v5
render: false
env_option:
    frame_skip: 4
    frame_stack: 4
    repeat_action_probability: 0.
    image_size: 84
    noop_max: 30
    terminal_on_life_loss: True
    grayscale_obs: True
wrapper_option:
  TransformReward: {f: "lambda reward: np.sign(float(reward))"}

# dqn setting
network_type: Default
network_config:
  Q_network:
    hidden_units: [64, 64]
  Encoder:
    feature_dim: 512
    layer_num: 3
    channel_num: [32, 64, 64]
    kernel_size: [8, 4, 3]
    strides: [4, 2, 1]
eps_initial: 1.0
eps_final: 0.01
decay_fraction: 0.1
gamma: 0.99
batch_size: 32
buffer_size: 100000
learning_rate: 0.0001
optimizer: Adam
optimizer_kwargs: {}
anneal_lr: False
target_copy_freq: 1000
device: cpu

# trainer setting
seed: 1
training_start: 80000
training_freq: 4
training_num: 1
max_step: 10000000
do_eval: true
eval_every: 10000
eval_ep: 1

# log setting
log_tensorboard: false
log_wandb: false
record_video: false
save_model: false
save_freq: 100000

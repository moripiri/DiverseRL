env_id: HalfCheetah-v4
render: false
env_option: {}
wrapper_option:
  ClipAction: {}
  NormalizeObservation: {}
  TransformObservation: {"f": "lambda obs: np.clip(obs, -10, 10)"}
  NormalizeReward: {"gamma": 0.99}
  TransformReward: {"f": "lambda reward: np.clip(reward, -10, 10)"}
num_envs: 1

network_type: Default
network_config:
  Actor:
    hidden_units: [64, 64]
    mid_activation: "Tanh"
    kernel_initializer: "orthogonal_"
    kernel_initializer_kwargs: {"gain": "np.sqrt(2)"}
    bias_initializer: "constant_"
    bias_initializer_kwargs: {"val": 0.0}
  Critic:
    hidden_units: [64, 64]
    mid_activation: "Tanh"
    kernel_initializer: "orthogonal_"
    kernel_initializer_kwargs: {"gain": np.sqrt(2)}
    bias_initializer: "constant_"
    bias_initializer_kwargs: {"val": "0.0"}
mode: clip
clip_coef: 0.2
target_dist: 0.01
beta: 3.0
gamma: 0.99
lambda_gae: 0.95
vf_coef: 0.5
entropy_coef: 0.0
horizon: 2048
minibatch_size: 64
num_epochs: 10
learning_rate: 0.0003
optimizer: Adam
optimizer_kwargs: {"eps": 0.00001}
anneal_lr: true
max_grad_norm: 0.5
device: cpu
seed: 1

max_step: 2000000
do_eval: false
eval_every: 10000
eval_ep: 1
log_tensorboard: true
log_wandb: true
record_video: false
save_model: false
save_freq: 100000

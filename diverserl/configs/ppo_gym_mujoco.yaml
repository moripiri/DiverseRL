defaults:
  - env: gym_mujoco
  - algo: ppo
  - trainer: on_policy_trainer

env:
  env_id: HalfCheetah-v5
  env_option: {}
  wrapper_option:
    ClipAction: {}
    NormalizeObservation: {}
    TransformObservation: { "func": "lambda obs: np.clip(obs, -10, 10)", "observation_space": "env.observation_space"}
    NormalizeReward: { "gamma": 0.99 }
    TransformReward: { "func": "lambda reward: np.clip(reward, -10, 10)" }
  seed: ${trainer.seed}
  num_envs: 1
  vector_env: true
  render: false
  record: false
algo:
  network_type: Default
  network_config:
    Actor:
      hidden_units: [ 64, 64 ]
      mid_activation: "Tanh"
      kernel_initializer: "orthogonal_"
      kernel_initializer_kwargs: { "gain": "np.sqrt(2)" }
      bias_initializer: "constant_"
      bias_initializer_kwargs: { "val": 0.0 }
    Critic:
      hidden_units: [ 64, 64 ]
      mid_activation: "Tanh"
      kernel_initializer: "orthogonal_"
      kernel_initializer_kwargs: { "gain": np.sqrt(2) }
      bias_initializer: "constant_"
      bias_initializer_kwargs: { "val": "0.0" }
  horizon: 2048
  minibatch_size: 64
  num_epochs: 10
  gamma: 0.99
  lambda_gae: 0.95
  mode: clip
  target_dist: 0.01
  beta: 3.0
  clip_coef: 0.2
  vf_coef: 0.5
  entropy_coef: 0.0
  max_grad_norm: 0.5
  learning_rate: 0.0003
  optimizer: Adam
  optimizer_kwargs: { "eps": 0.00001 }
  anneal_lr: true
  device: cpu
trainer:
  seed: 1234
  max_step: 1000000
  do_eval: true
  eval_every: 10000
  eval_ep: 1
  record: ${env.record}
  log_tensorboard: false
  log_wandb: false
  save_model: false
  save_freq: 100000

hydra:
  run:
    dir: .
  output_subdir: null
